# AI Resources

Some resources to start out with (these are not strict — more to give you options to shop around with; please look at other resources if they interest you).

---

## Courses & Guided Tracks
- [3B1B Neural Networks intro](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Taking Andrew Ng's ML specialization course](https://www.deeplearning.ai/courses/machine-learning-specialization/)

---

## Lectures, Talk Series & YouTube Channels
- [Andrej Karpathy — watching many of his videos (includes "GPT from scratch")](https://www.youtube.com/andrejkarpathy)
- [Pieter Abbeel lectures](https://www.youtube.com/@PieterAbbeel/playlists)
- [MIT AI lectures](https://m.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) 
- [Scaling talk (Stanford Online)](https://www.youtube.com/watch?v=orDKvo8h71o&ab_channel=StanfordOnline)
- [Watching some of Yannic Kilcher's videos is also useful](https://www.youtube.com/@YannicKilcher)

---

## How to Read / Paper Lists
- [How to read papers (fast.ai forum)](https://forums.fast.ai/t/how-to-read-research-papers-andrew-ng/66892) 
- [Ilya Sutskever list of papers/topics](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE) 
- [Reddit thread: "The ML papers that rocked our world 2020–2023"](https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/)

---

## Core Papers
- [ResNet](https://arxiv.org/abs/1512.03385)
- [VAEs](https://arxiv.org/abs/1312.6114)
- [GANs](https://arxiv.org/pdf/1406.2661)
- [Transformers](https://arxiv.org/abs/1706.03762)
- [VQ-VAE](https://arxiv.org/abs/1711.00937)
- [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Vision Transformers](https://arxiv.org/abs/2010.11929)
- [Scaling Laws](https://arxiv.org/abs/2001.08361)


---

## Self-Supervised Learning (SSL)
- [Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748)
- [CLIP](https://arxiv.org/abs/2103.00020)
- [MAE](https://arxiv.org/abs/2111.06377)
- [DINO](https://arxiv.org/pdf/2104.14294)
- [SSL Cookbook](https://arxiv.org/pdf/2304.12210)
- [Yann Lecun's Jepa](https://openreview.net/pdf?id=BZ5a1r-kVsf)

---

## World Models / Planning
- [World Models](https://arxiv.org/pdf/1803.10122)
- [Dreamerv1](https://arxiv.org/pdf/1912.01603), [Dreamerv3](https://arxiv.org/abs/2301.04104)
- [Dino-WM](https://arxiv.org/abs/2411.04983)
- [Diffuser](https://arxiv.org/abs/2205.09991)

---

## Diffusion (videos + papers)
- [Diffusion explainer — Umar Jamil](https://www.youtube.com/watch?v=ZBKpAp_6TGI&ab_channel=UmarJamil)
- [Jeremy Howard talk (part 1)](https://www.youtube.com/watch?v=_7rMfsA24Ls&t=7035s&ab_channel=JeremyHoward)  
- [Jeremy Howard talk (part 2)](https://www.youtube.com/watch?v=6StU6UtZEbU&t=1194s&ab_channel=JeremyHoward) 
- [Diffusion paper A](https://arxiv.org/pdf/2112.10752)
- [Diffusion paper B](https://arxiv.org/pdf/2212.09748)
- [MIT Diffusion models intro](https://www.youtube.com/watch?v=EUNr6Z_xbCA&list=PL_1TbuIu65A_G908tHHvTnyQsueR17rMh&index=4)
- [Flow = Diffusion](https://diffusionflow.github.io/)

---

## Energy-Based Models (videos + papers)
- [Energy-Based Transformers covered by yannic kilcher](https://www.youtube.com/watch?v=RAEy3JZmIaA)
- [Energy-Based Transformers covered by bycloud](https://www.youtube.com/watch?v=LUQkWzjv2RM)
- [Energy-Based Transformers covered by AI Coffe Break](https://www.youtube.com/watch?v=18Fn2m99X1k)
- [Energy-Based Transformers](https://energy-based-transformers.github.io/), (see sections H and I for an easy to understand intro)
- [Implicit Generation and Generalization](https://arxiv.org/pdf/1903.08689)
- [Iterative reasoning as energy minimization](https://arxiv.org/pdf/2206.15448)
- [Iterative reasoning as energy diffusion](https://arxiv.org/pdf/2406.11179)
- [EBMs for compositional generalization](https://arxiv.org/abs/2302.11552)

---

## Communities, News & Social
- [Twitter (X) — follow larger accounts I follow including Yann LeCun, AK, etc](https://x.com/AlexiGlad/following)
- [Reddit — r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

---

## Blogs & Practical Guides
- [Karpathy — debugging/training ("Recipe")](https://karpathy.github.io/2019/04/25/recipe/) 
- [Google Research — hparam tuning playbook](https://github.com/google-research/tuning_playbook#who-is-this-document-for)
- [Lilian Weng](https://lilianweng.github.io/)
- [Tim Dettmers](https://timdettmers.com/)
- [Karpathy (home)](https://karpathy.github.io/)
- [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

---

## Implementation / Hands-on
- Coding models in PyTorch ([stable diffusion](https://www.youtube.com/watch?v=ZBKpAp_6TGI&ab_channel=UmarJamil), [LLM transformer](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=41s&ab_channel=AndrejKarpathy))
- Code [DiT architecture](https://github.com/facebookresearch/DiT) from scratch based on the [paper](https://arxiv.org/abs/2212.09748) using only the trainer and diffusion module (no using LLMs for help)

---

## Notes & Advice
- Rewatch/re-read things you didn't get initially after some time — progress is real but gradual.  
- Use ChatGPT / LLMs to help explain papers and math (use cautiously; don't over-rely).  
- My blog on how to do AI research (coming soon — feel free to ask me for the draft)
